from pyspark.sql import DataFrame
from pyspark.sql.functions import col
import traceback

def write_clickhouse_batch(
    df: DataFrame, 
    batch_id: int,
    table_name: str, 
    host: str = "clickhouse", 
    port: int = 8123, 
    user: str = "default", 
    password: str = "12345",
    database: str = "default"
) -> None:
    """
    Ghi t·ª´ng micro-batch c·ªßa streaming DataFrame v√†o ClickHouse.
    """
    
    try:
        print(f"\n{'='*60}")
        print(f"[Batch {batch_id}] Starting to process...")
        print(f"{'='*60}")
        
        # Ki·ªÉm tra batch r·ªóng
        count = df.count()
        if count == 0:
            print(f"[Batch {batch_id}] Empty batch, skipping...")
            return
        
        print(f"[Batch {batch_id}] Processing {count} records")
        
        # Print schema l·∫ßn ƒë·∫ßu
        if batch_id == 0:
            print("\nüìä DataFrame Schema:")
            df.printSchema()
        
        # Show sample data
        print(f"\n[Batch {batch_id}] Sample data (3 rows):")
        df.show(3, truncate=False)
        
        # ƒê·∫¢M B·∫¢O TH·ª® T·ª∞ C·ªòT ƒê√öNG V·ªöI CLICKHOUSE TABLE
        df_ordered = df.select(
            "Symbol",
            "TradeID",
            "Price",
            "Quantity",
            "EventTime",
            "TradeTime",
            "IsBuyerMaker",  # ƒê√£ l√† Int (0/1)
            "Side",
            "TradeValue",
            "Year",
            "Month",
            "Day",
            "Hour"
        )
        
        # Verify data types
        if batch_id == 0:
            print("\nüîç Column types after ordering:")
            for field in df_ordered.schema.fields:
                print(f"  {field.name}: {field.dataType}")
        
        clickhouse_url = f"jdbc:clickhouse://{host}:{port}/{database}"
        
        connection_properties = {
            "user": user,
            "password": password,
            "driver": "com.clickhouse.jdbc.ClickHouseDriver",
            "batchsize": "10000",
            "socket_timeout": "300000",
            "connect_timeout": "60000",
            "rewriteBatchedStatements": "true"
        }
        
        print(f"\n[Batch {batch_id}] üì§ Writing to ClickHouse...")
        print(f"  URL: {clickhouse_url}")
        print(f"  Table: {table_name}")
        print(f"  Records: {count}")
        
        # Ghi v√†o ClickHouse
        df_ordered.write \
            .jdbc(
                url=clickhouse_url,
                table=table_name,
                mode="append",
                properties=connection_properties
            )
        
        print(f"\n[Batch {batch_id}] ‚úÖ SUCCESS! Wrote {count} records to ClickHouse")
        print(f"{'='*60}\n")
        
    except Exception as e:
        print(f"\n[Batch {batch_id}] ‚ùå ERROR writing to ClickHouse:")
        print(f"{'='*60}")
        print(f"Error type: {type(e).__name__}")
        print(f"Error message: {str(e)}")
        print(f"{'='*60}")
        traceback.print_exc()
        
        # KH√îNG raise exception ƒë·ªÉ stream kh√¥ng b·ªã d·ª´ng
        # N·∫øu mu·ªën d·ª´ng stream khi c√≥ l·ªói, uncomment d√≤ng d∆∞·ªõi:
        # raise