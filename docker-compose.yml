version: "3.8"

services:
  # ==================== ZOOKEEPER ====================
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  # ==================== KAFKA ====================
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "127.0.0.1:9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181

      KAFKA_LISTENERS: INTERNAL://0.0.0.0:9092,EXTERNAL://0.0.0.0:19092
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:9092,EXTERNAL://host.docker.internal:19092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL

      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_NUM_PARTITIONS: 3

    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "9092"]
      interval: 5s
      timeout: 10s
      retries: 10
    restart: unless-stopped
  # ==================== KAFKA UI ====================
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    ports:
      - "8085:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181

  # ==================== CLICKHOUSE (DB) ====================
  clickhouse:
    image: clickhouse/clickhouse-server:latest
    container_name: clickhouse
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    ports:
      - "8123:8123" # HTTP
      - "9000:9000" # Native
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - clickhouse_logs:/var/log/clickhouse-server
    environment:
      CLICKHOUSE_DB: default
      CLICKHOUSE_USER: default
      CLICKHOUSE_PASSWORD: ""

  # ==================== SPARK MASTER ====================
  spark-master:
    image: apache/spark:3.5.0
    container_name: spark-master
    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      - SPARK_MODE=master
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master

  # ==================== SPARK WORKER ====================
  spark-worker:
    image: apache/spark:3.5.0
    container_name: spark-worker
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
    ports:
      - "8081:8081"
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077

  # ==================== SPARK SUBMIT ====================
  spark-submit:
    image: apache/spark:3.5.0
    container_name: spark-submit
    user: root
    depends_on:
      - kafka
      - spark-master
      - clickhouse
    volumes:
      - ./app:/app
      - ./jars:/extra-jars          # không ghi đè thư mục jars của Spark
    environment:
      SPARK_MASTER_URL: spark://spark-master:7077
    command: >
      /opt/spark/bin/spark-submit
        --master spark://spark-master:7077
        --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0
        --jars /extra-jars/clickhouse-spark-runtime-3.5_2.12-0.8.1.jar
        --conf spark.driver.memory=2g
        --conf spark.executor.memory=2g
        /app/spark_streaming.py

  # ==================== PRODUCER (PYTHON) ====================
  producer:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: producer
    depends_on:
      kafka:
        condition: service_healthy
    volumes:
      - ./data:/data
      - ./producer:/app
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - PYTHONUNBUFFERED=1
    command: >
      bash -c "
        echo 'Checking packages...' &&
        python -c 'import confluent_kafka, websockets, json; print(\"All OK\")' &&
        echo 'Waiting for Kafka to be fully ready...' &&
        sleep 10 &&
        echo 'Starting producers...' &&
        python /app/producer.py
      "

volumes:
  clickhouse_data:
  clickhouse_logs:
  spark-ivy-cache:
  spark-checkpoints:
